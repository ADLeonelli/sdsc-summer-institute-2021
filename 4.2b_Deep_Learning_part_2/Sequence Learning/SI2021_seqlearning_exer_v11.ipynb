{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIML Sequence Learning exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:**  Use simpleRNN, and/or GRU, to learn Keras api and options.  \n",
    "\n",
    "**Problem:** Given a sequence of a real variable and binary indicator, \n",
    "              add real values when indicator=1\n",
    "\n",
    "**Exercise Tasks:**\n",
    "<br>\n",
    "1\n",
    "run notebook and look at graph (near end) with **model mse vs baseline (mean)** to see performance\n",
    "<br>\n",
    "2 change options that make up data:\n",
    "<br>\n",
    "$\\;\\;\\;\\;\\;\\;$ a. set the \"t-fixed\" boolean to False so that the time steps when indicator=1 will be random\n",
    "<br>\n",
    "$\\;\\;\\;\\;\\;\\;$ b. also, set num1=4 (number of ones=4, for example) \n",
    "<br>\n",
    "$\\;\\;\\;\\;\\;\\;$ c. RERUN \n",
    "<br>\n",
    "3 \n",
    "a. find the code that defines the SimpleRNN model, copy it to the next cell, and change it to use GRU\n",
    "<br>\n",
    "b. find the code cell that compile/fits the model (ie look for comments with \"<<<<----\" in the text), change the model to use 'GRU', and rerun the notebook\n",
    "<br>\n",
    "Is there a difference between SRN or GRU vs baseline?\n",
    "<br>\n",
    "What if you increase number of hidden units or layers on one or both?\n",
    "<br>\n",
    "<br>\n",
    "**Other things to try (with more time):**\n",
    "\n",
    "*change the logic that makes the target:* \n",
    "<br>\n",
    " try summing the numbers that is 1,2 or 3 steps before when indicator=1 (look for nback variable below) \n",
    "\n",
    "*change input to use the random sine waves: (see seqlearning_timeseries notebook )*\n",
    "<br>\n",
    " the target to be the input one step ahead but you have to change the model definition to use TimeDistributed option\n",
    " <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT STATEMENTS\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(1)  # for reproducibility\n",
    " \n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next cell has the sequence options to choose**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================options to change =================\n",
    "samplesize_2use=500;  #pick a large enough size\n",
    "nsteps_2use    =20;    #this is T stpes\n",
    "\n",
    "                       #<<<<<<<<<<<------------------\n",
    "num1       =2          #choose number of 1s for binary variable\n",
    "t_fixed    =True       #True or False,  are the times when it is 1 fixed or variable\n",
    "t_fix_inds =[5,15]     #  the times to set it to 1\n",
    "\n",
    "#Now set up parameters to run model   \n",
    "numunits       =64\n",
    "act2use        ='relu'  #or relu or sigmoid or tanh\n",
    "\n",
    "print('options done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next cell generates binary indicator variable that is 1 according to options**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate a binary variable that is 1 exactly num1 times in each sample\n",
    "#  the time steps to set it to 1 are either fixed or randomly chosen for each sample\n",
    "\n",
    "nback      =0  #steps to look backward when adding\n",
    "binary_num1=np.zeros((samplesize_2use,nsteps_2use,1))\n",
    "\n",
    "if t_fixed:\n",
    "    for i in range(len(t_fix_inds)):\n",
    "       binary_num1[:,t_fix_inds[i]]=1\n",
    "if not t_fixed:\n",
    "    for i in range(samplesize_2use):             \n",
    "      choices_fornum1 = np.random.choice(np.arange(nsteps_2use-nback), size=num1, replace=False, p=None) #pick out num1 indices                                      \n",
    "      for j in choices_fornum1:\n",
    "          binary_num1[i,j+nback]=1     \n",
    "\n",
    "#plt.plot(binary_num1[1,:],'.')  #if you want to see the sample\n",
    "print('binary variable generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sanity check: this will help check that each raw has right number of 1s\n",
    "print(max(np.sum(binary_num1,axis=1))  )\n",
    "print(min(np.sum(binary_num1,axis=1))  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next cells generate random sequence of numbers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----- Generate sequence of random numbers  - \n",
    "def generate_random_series(batch_size, n_steps, noise_amp):\n",
    " series = noise_amp * (np.random.rand(batch_size, n_steps) ) # + noise that is uniform dist. between 0 to 1\n",
    " return series[..., np.newaxis].astype(np.float32)\n",
    "\n",
    "noiseAmp_2use    =1;\n",
    "my_randnum_seq   =generate_random_series(samplesize_2use,nsteps_2use,noiseAmp_2use)\n",
    "print(my_randnum_seq.shape)\n",
    "#plt.plot(my_randnum_seq[1,:,0])\n",
    "#plt.title('a sample time series of random values unif dist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next cell make the Y target values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now combine randnum seq,binary_num1 and make up Y values\n",
    "X_train=np.concatenate((my_randnum_seq,binary_num1),axis=2)\n",
    "N,T,P   =X_train.shape\n",
    "\n",
    "Y_train =np.zeros((N,))  #1 output at each step\n",
    "\n",
    "for i in range(N):\n",
    "    locs       = np.where(X_train[i,:,1]==1)        #column 1 of X_train has indicator    \n",
    "    Y_train[i,]= np.sum(X_train[i,locs[0]-nback,0])     \n",
    "\n",
    "ymean     =np.mean(Y_train[:,])\n",
    "ymean_mse = np.mean(np.square(np.subtract(Y_train,ymean)))\n",
    "plt.plot(Y_train[:,],'.')\n",
    "plt.ylabel('true value')\n",
    "plt.xlabel('data sample')\n",
    "plt.title('Y train targets, mean'+str(round(ymean,5))+' mean mse:'+str(round(ymean_mse,5)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to see one sample time series\n",
    "plt.plot(X_train[0,:,0])\n",
    "plt.plot(X_train[0,:,1])\n",
    "plt.legend(['X random','X indicator'])\n",
    "plt.title('sample data, Y target:'+str(np.round(Y_train[0],2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The next cells set up the model, do training, plot results\n",
    "## Review the results plot and note the performance relative to baseline mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train1, X_test1,Y_train1, Y_test1= train_test_split(X_train,Y_train, test_size=0.33) #use 33% for test data\n",
    "\n",
    "print(X_train1.shape)\n",
    "Y_train1.shape\n",
    "print(Y_test1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a Simple RNN setup  \n",
    "#set return_sequences=True for all recurrent layers\n",
    "#except the last one, if you only care about the last output\n",
    "nvar           = P   #set number of variables to P \n",
    "\n",
    "mysrn_model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(numunits, return_sequences=True, input_shape=[None,nvar]),\n",
    "    keras.layers.SimpleRNN(numunits),\n",
    "    keras.layers.Dense(1,activation='linear')])     \n",
    "\n",
    "mysrn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add in the GRU definition here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a 2 layer GRU RNN; copy the code for that defines the SRN model and change\n",
    "#  keras.layers.SimpleRNN   to   keras.layers.GRU\n",
    "\n",
    "#mygru_model = keras.models.Sequential([  .........\n",
    " \n",
    "        #<<<<<<<<<<<<<<---------------------- \n",
    "\n",
    "#mygru_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function to be 'called' back by keras during training\n",
    "my_early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_mse', \n",
    "    verbose=1,\n",
    "    patience=10,   #num  epochs with no improvement after which training will be stopped.\n",
    "    mode='min',\n",
    "    restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numepochs=100\n",
    "\n",
    "my_model=mysrn_model   #<<<<<<<<<----- you can change the model here and run these cells again\n",
    "#my_model=mygru_model   \n",
    "\n",
    "my_model.compile(optimizer='adam',  # just use 'adam' to get defaults\n",
    "                loss='mse',                  \n",
    "                metrics='mse')                \n",
    "\n",
    "fit_result = my_model.fit(X_train1,y=Y_train1,\n",
    "                    validation_data=(X_test1,Y_test1),\n",
    "                    epochs=numepochs,batch_size=32,verbose=1,callbacks=[my_early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot performance \n",
    "print('scores on validation data:')\n",
    "mytest_eval=my_model.evaluate(X_test1,Y_test1)\n",
    "\n",
    "metric2plot='mse'\n",
    "# summarize history for accuracy\n",
    "plt.figure()\n",
    "plt.plot(fit_result.history[metric2plot])\n",
    "plt.plot(fit_result.history['val_'+metric2plot])\n",
    "plt.title(my_model.layers[0].name+'; '+str(num1)+' #1s; fixed:'+str(t_fixed)+' nback:'+str(nback)+'; '+metric2plot+' over training;'+ ' basemse: '+str(np.around(ymean_mse,5)))\n",
    "plt.ylabel(metric2plot)\n",
    "plt.xlabel('epoch')\n",
    "#plt.ylim([0,1])\n",
    "plt.legend(['train', 'valid'], loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some code to get training and test set predictions\n",
    "#(just for exploring performance)   unblock it if you want to try it\n",
    "if 0:\n",
    " mytrain_pred=my_model.predict(X_train1[:,:,:])\n",
    " plt.plot(Y_train1,'.')\n",
    " plt.plot(mytrain_pred,'.')\n",
    " plt.ylabel('value')\n",
    " plt.xlabel('nth data sample')\n",
    " plt.legend(['train,targets', 'predictions'], loc='lower left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "  mytest_pred=my_model.predict(X_test1[:,:,:])\n",
    "  plt.plot(Y_test1,'.')\n",
    "  plt.plot(mytest_pred,'.')\n",
    "  plt.ylabel('value')\n",
    "  plt.xlabel('nth data sample')\n",
    "  plt.legend(['test,targets', 'predictions'], loc='lower left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
